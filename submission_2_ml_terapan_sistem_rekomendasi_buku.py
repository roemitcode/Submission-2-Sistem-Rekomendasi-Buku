# -*- coding: utf-8 -*-
"""Submission_2_ML Terapan_Sistem Rekomendasi Buku.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GLKbzVaNLCZxe_JGcBQl4BeKCEmbXeln

# Deskripsi Proyek ML
Sistem rekomendasi buku ini akan dibangun menggunakan algoritma machine learning content based filtering yang dapat belajar dari dataset buku yang diperoleh dari kaggle. Dengan menganalisis deskripsi judul buku dan rating dari data set, sistem akan menghasilkan rekomendasi yang lebih personal dan akurat. Tujuan utama proyek ini adalah meningkatkan pengalaman membaca pengguna dengan menyajikan buku-buku yang sesuai dengan minat mereka.

# Tahap 1 Instalansi dan Persiapan Library
  Pada tahap, dilakukan persiapan semua pustaka Python yang diperlukan seperti pandas yang digunakan untuk untuk manipulasi data maupun matplotlib untuk visualisasi data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tabulate import tabulate
import seaborn as sns
from sklearn.metrics import f1_score

"""#Tahap 2: Data Loading

Pada tahap ini akan dilakukan pemuatan dataset dari kaggle (https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/data). Dataset yang dimuat ke dalam Google Colabs berasal dari sumber eksternal seperti file CSV. Proses ini memastikan data tersedia dalam format yang dapat diproses. Jumlah dan struktur data dilihat dengan menggunakan fungsi dengan head(). Dataset ini mengandung kolom seperti book_id, title ataupun author.
"""

# Tahap 2: Data download and Loading
# Unduh dataset
!pip install kaggle  # Pastikan Kaggle CLI terpasang

#Mount ke Google Drive
from google.colab import drive
drive.mount('/content/drive')

#Pindahkan token API Kaggle ke direktori kerja Colab
!mkdir ~/.kaggle
!cp /content/drive/MyDrive/Kaggle_API/kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

 #Download Public Kaggle Dataset
!kaggle datasets download -d arashnic/book-recommendation-dataset -p ./data/

# Ekstrak dataset
import zipfile
with zipfile.ZipFile('./data/book-recommendation-dataset.zip', 'r') as zip_ref:
    zip_ref.extractall('./data/')

books = pd.read_csv("/content/data/Books.csv")
books.head()

ratings = pd.read_csv("/content/data/Ratings.csv")
ratings.head()

print('Jumlah buku dalam data:', len(books.ISBN.unique()))
print('Jumlah rating buku dalam data :', len(ratings.ISBN.unique()))

"""# Tahap 3 : Eksploration Data Analysis (EDA)

Sebelum melanjutkan ke model, kita perlu memahami distribusi data dan memeriksa karakteristik data. Selain itu pada tahap ini akan dilakukan analaisis jikalau ada data yang kurang baik untuk diproses dalam model.

# Tahap 3.1. EDA : Dataset Buku

Pada tahap ini akan dilakukan analisis dan perbaikan pada dataset buku
"""

books.info()

"""Tampak pada kolom Years-Of-Publication data type nya object padahal seharusnya integer, sehingga perlu dibenahi"""

temp = (books['Year-Of-Publication'] == 'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')
books = books.drop(books[temp].index)
books[(books['Year-Of-Publication'] == 'DK Publishing Inc') | (books['Year-Of-Publication'] == 'Gallimard')]

books['Year-Of-Publication'] = books['Year-Of-Publication'].astype(int)
print(books.dtypes)

"""Sekarang, kita akan menampilkan 20 buku teratas berdasarkan jumlah buku yang dihasilkan penulis"""

# Penggabungan 'Book-Author'
# menghitung karya buku si penulis
author_counts = books.groupby('Book-Author')['Book-Title'].count()

# Urutkan penulis dari yang banyak hingga yang sedikti
sorted_authors = author_counts.sort_values(ascending=False)

# 20 penulis teratas
top_20_authors = sorted_authors.head(20)

# Plot 20 penulis teratas dengan plot batang dan warna
plt.figure(figsize=(12, 6))
colors = plt.cm.viridis(top_20_authors.values / float(max(top_20_authors.values)))
top_20_authors.plot(kind='bar', color=colors)
plt.xlabel('Nama Pena Penulis')
plt.ylabel('Jumlah Karya Buku')
plt.title('20 Penulis Karya Buku Terbanya')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""Wow...Agatha Christie menjadi penulis dengan karya terbanyak sebesar 600 lebih buku"""

# Visualisasi jumlah buku berdasarkan penerbit
plt.figure(figsize=(12, 6))
books['Publisher'].value_counts().head(20).plot(kind='bar', color=colors)
plt.title('Jumlah Buku per Penerbit (Top 20)')
plt.xlabel('Penerbit')
plt.ylabel('Jumlah Buku')
plt.xticks(rotation=45, ha='right')
plt.show()

"""# 3.2. EDA : Dataset Ratings

Pada tahapan selanjutnya dilakukan eksplorasi pada dataset ratings. Dataset rating ini juga nantinya akan digunakan untuk sistem rekomendasi buku dengan model collaborative filtering
"""

ratings.info()

"""Kemudian, akan dilihat jumlah data dari masing-masing kolom"""

print('Banyaknya User-ID:', len(ratings['User-ID'].unique()))
print('Banyaknya Karya Buku yang ber-ISBN:', len(ratings['ISBN'].unique()))

print('Jumlah 10 teratas dari rating buku:')
sorted_ratings = ratings['Book-Rating'].value_counts().sort_index()
pd.DataFrame({'Book-Rating': sorted_ratings.index, 'Jumlah': sorted_ratings.values})

"""Pada pengembangan model ini hanya akan digunakan 20000 data saja untuk menghemat waktu dalam menjalankan model"""

df_rating = ratings[:20000]
df_rating

# Visualisasi distribusi rating buku (menggunakan data yang sudah di-subset)
plt.figure(figsize=(8, 6))
sns.histplot(df_rating['Book-Rating'], kde=True)
plt.title('Distribusi Rating Buku (20.000 data pertama)')
plt.xlabel('Rating')
plt.ylabel('Jumlah Rating')
plt.show()

# Visualisasi jumlah rating per rating buku (menggunakan data yang sudah di-subset)
plt.figure(figsize=(8, 6))
df_rating['Book-Rating'].value_counts().sort_index().plot(kind='bar')
plt.title('Jumlah Rating per Nilai Rating (20.000 data pertama)')
plt.xlabel('Rating Buku')
plt.ylabel('Jumlah Rating')
plt.show()

"""# Tahap 4 : Data Preprocessing
 Pada tahap ini, 2 file terpisah yaitu books dan ratings, akan digabungankan menjadi 1 file. Hal ini dilakukan agar sesuai dengan pengembangan model.
"""

# Menggabungkan dataframe ratings dengan books berdasarkan nilai ISBN
books = pd.merge(ratings, books, on='ISBN', how='left')
books

books.groupby('ISBN').sum()

"""# Tahap 5 : Data Preparation
Di tahap, akan dilakukan persiapan data seperti memeprbaiki missing value dan menyamakan jenis buku berdasarkan ISBN.
Pada sistem rekomendasi berbasis konten (content-based filtering) yang akan dikembangkan, satu nomor ISBN mewakili satu judul buku.
"""

# Cek missing value dengan fungsi isnull()
books.isnull().sum()

"""Terdapat mising value dari Book-Title sampai Image-URL-L sehingga perlu untuk diperbaiki"""

all_books_clean = books.dropna()
all_books_clean

"""Diperiksa kembali apakah ada misisng value"""

all_books_clean.isnull().sum()

# Mengurutkan buku berdasarkan ISBN kemudian memasukkannya ke dalam variabel fix_books
fix_books = all_books_clean.sort_values('ISBN', ascending=True)
fix_books

"""Dihitung jumlah nomor isbn yang ada dalam dataset"""

# Mengecek berapa jumlah fix_books
len(fix_books['ISBN'].unique())

"""Diperiksa kembali jumlah buku"""

len(fix_books['Book-Title'].unique())

"""Jumlah buku dan banyaknya ISBN tidak sama sehingga perlu diatasi dengan mengubah datasetnya menjadi data unik. Selain itu, diperlukan proses penghilangan data duplikat pada kolom 'ISBN' dan simpan ke dalam variabel baru."""

preparation = fix_books.drop_duplicates('ISBN')
preparation

"""Kemudian dilakukan proses konversi data series menjadi list dengan fungsi tolist() dari library."""

# konversi data series 'ISBN' menjadi bentuk list
isbn_id = preparation['ISBN'].tolist()

# konversi data series 'Book-Title' menjadi bentuk list
book_title = preparation['Book-Title'].tolist()

# konversi data series 'Book-Author' menjadi bentuk list
book_author = preparation['Book-Author'].tolist()

# konversi data series 'Year-Of-Publication' menjadi bentuk list
year_of_publication = preparation['Year-Of-Publication'].tolist()

# konversi data series 'Publisher' menjadi bentuk list
publisher = preparation['Publisher'].tolist()

print(len(isbn_id))
print(len(book_title))
print(len(book_author))
print(len(year_of_publication))
print(len(publisher))

"""Tahap berikutnya yaitu pembuatan dictionary untuk menentukan pasangan key-value pada data isbn_id, book_title, book_author, year_of_publication, dan publihser"""

# Membuat dictionary untuk data ‘isbn_id’, ‘book_title’, ‘book_author’, 'year_of_publication', dan 'publisher'
books_new = pd.DataFrame({
    'isbn': isbn_id,
    'book_title': book_title,
    'book_author': book_author,
    'year_of_publication': year_of_publication,
    'publisher': publisher

})

books_new

"""Untuk mengefisienkan waktu dalam permodelan, maka data yang akan digunakan hanya sampai pada urutan 20.000"""

books_new = books_new[:20000]

books_new

"""# Tahap 6 Pembuatan Model: Content-Based Filtering

Pada tahap ini dilakukan pencarian representasi fitur penting dari setiap judul buku dengan TF-IDF (Term Frequency - Inverse Document Frequency) Vectorizer. Tahap ini juga akan Filtering menggunakan Cosine Similarity untuk menghitung derajat kesamaan antar buku.
"""

data = books_new
data.sample(5)

"""# Tahap 6.1: TF-IDF Vectorizer
TF-IDF vectorizer adalah alat yang digunakan untuk mengonversi dokumen teks menjadi representasi vektor berdasarkan nilai TF-ID setiap kata dalam dokumen tersebut.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data book_author
tf.fit(data['book_author'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['book_author'])

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# matriks tf-idf untuk beberapa judul buku dan nama penulis buku dalam bentuk dataframe
pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data.book_title
).sample(15, axis=1).sample(10, axis=0)

"""# Tahap 6.2 : Cosine Similarity
Pada tahap ini dilakukan proses menghitung derajat kesamaan (similarity degree) antar judul buku dengan teknik cosine similarity.
"""

from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama judul buku
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['book_title'], columns=data['book_title'])
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap judul buku
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""# tahap 7 : Hasil Sistem Rekomendasi Buku

Pada tahap ini, dibuatlah sebuah sistem rekomendasi buku dengan beberapa parameter book_title, similarity_data,
items dan k yang merupakan top-N recommendation yang diberikan oleh sistem rekomendasi. Secara default, k itu sendiri bernilai 5.
"""

def book_recommendation(book_title, similarity_data=cosine_sim_df, items=data[['book_title', 'book_author']], k=5):
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,book_title].to_numpy().argpartition(range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop book_title agar nama buku yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(book_title, errors='ignore')

    return pd.DataFrame(closest).merge(items).head(k)

book_title_test = "Entering the Silence : Becoming a Monk and a Writer (The Journals of Thomas Merton, V. 2)"
data[data.book_title.eq(book_title_test)]

"""# Tahap 8 : Evaluasi Model
 Evaluasi dilakukan dengan menggunakan MSE dan RMSE sebagai  evaluasi untuk sistem rekomendasi. Nilai dari MSE dan RMSE memberikan gambaran yang lebih seimbang tentang performa model. MSE (Mean Squared Error) dan RMSE (Root Mean Squared Error) adalah metrik evaluasi yang umum digunakan untuk mengukur seberapa baik suatu model prediksi mendekati nilai sebenarnya.
"""

# Menentukan threshold untuk mengkategorikan similarity sebagai 1 atau 0
threshold = 0.5
# Membuat ground truth data dengan asumsi threshold
ground_truth = np.where(cosine_sim >= threshold, 1, 0)

# Menampilkan beberapa nilai pada ground truth matrix
ground_truth_df = pd.DataFrame(ground_truth, index=data['book_title'], columns=data['book_title']).sample(5, axis=1).sample(10, axis=0)

ground_truth_df

from sklearn.metrics import mean_squared_error

# Perhitungan MSE
mse = mean_squared_error(ground_truth, cosine_sim)
print(f"Mean Squared Error (MSE): {mse}")

# Perhitungan RMSE
rmse = np.sqrt(mse)
print(f"Root Mean Squared Error (RMSE): {rmse}")

"""Kesimpulan dari model yang dibuat adalah sebagai berikut."""

# Kesimpulan MSE dan RMSE

print("Kesimpulan:")
print(f"Nilai MSE sebesar {mse} dan RMSE sebesar {rmse} menunjukkan tingkat error dari model dalam memprediksi kesamaan antar buku.")
print("Nilai MSE menunjukkan rata-rata kuadrat dari selisih antara nilai prediksi dan nilai sebenarnya. Semakin kecil nilai MSE, semakin baik model.")
print("RMSE adalah akar kuadrat dari MSE, memberikan interpretasi yang lebih mudah dalam skala yang sama dengan data asli.")

# Analisis lebih lanjut (sesuaikan dengan nilai yang diperoleh)
if rmse < 0.5:
  print("RMSE yang rendah menunjukkan model memiliki performa yang baik dalam memprediksi kesamaan buku.")
elif rmse < 1:
  print("RMSE menunjukkan akurasi model yang cukup baik, namun masih ada potensi perbaikan.")
else:
  print("RMSE yang tinggi menunjukkan bahwa model masih memiliki error yang signifikan dalam memprediksi kesamaan antar buku.")
  print("Perlu dilakukan evaluasi lebih lanjut, seperti penyesuaian parameter model atau eksplorasi fitur lain.")